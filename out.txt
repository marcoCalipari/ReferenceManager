
Simulation approaches can be divided into two main categories: (i) semantic model-based simulations that emphasize high-level relationships between entities in smart environments using abstract models to simulate activities and interactions; and (ii) physical model-based simulations that focus on replicating real-world physical processes, generating detailed, modality-specific data such as sensory readings. In semantic model-based simulations, relationships and functions between entities are manually set and implemented. Physical model-based simulations, on the other hand, calculate interactions based on physical laws and mathematical formulas for high-precision outcomes.  

In the physical level of IoT systems, semantic model-based simulation methods are developed for many different data types. For example, tools like OpenSHS [ASC17], Persim-3d [LCL15], and IE Sim [SCN14] are developed to analyze and generate datasets of Activities of Daily Living (ADL) for smart home applications, including lighting, security, and energy management. Semantic models are used to represent the relationships of entities at the physical level, network level and application level. They define functions for different interactions among the entities and thus, naturally synthesize different level data with the functions customized to specific relationships. On a larger scale, the Exploring [PY21] framework involves a task-based approach with the Digital Twin of a city where citizens are engaged in performing certain activities to create data that reflects hypothetical scenarios. A Digital Twin replicates real-world entities, in this case, a city. It integrates data from various sources to create a comprehensive model that can simulate and predict the physical existence of the city, including aspects like traffic flow, energy consumption, and environmental impact. 

The semantic model is also a powerful tool to model relationships in network components. Network Simulator 3 (ns-3) [RH10] is a discrete-event network simulator that simulates the behavior of various network protocols and configurations, providing a highly flexible environment for testing and development. Semantic model enhances ns-3 by offering a structured framework for simulating nodes, links, protocols, and applications. This integration allows users to simulate realistic network traffic data and analyze the performance of network configurations.  

Known for the robust capability to guide simulations on the physical and network levels, semantic models extend their utility to higher-level simulations involving application knowledge. This capability is particularly useful in complex IoT systems where different levels of data are integrated. A leading example is SmartSPEC [CJG22], a simulation tool that synthesizes realistic occupancy and customizable sensor data across both physical and network levels. It uses a four-component semantic model—Space, Event, Person, and Sensors—to capture intricate human activity patterns within a smart environment. SmartSPEC generates data based on Wi-Fi occupancy datasets and allows full customization of data generation functions. Its key feature is the ability to learn event distributions from seed data, enabling it to extract abstract information about events and activities, thus transforming raw data into application-level insights. 

Physical model simulation involves creating detailed, often mathematical models of physical systems to understand and predict their behavior under various conditions. To synthesize sensor data, physical-based methods create high-precision simulations of specific phenomena like acoustic signals (e.g., Soundspaces 2.0 [CSG22]) or point clouds (Lidarsim [MWW20]). These methods are based on well-defined physical scenarios, including the environment, and physical data of entities (3D model, surface texture and parameters, etc.) to simulate physical sensory data or network reflection data. Thus, these methods are used as a simulator from the physical level to sensor data or even the network level, while the high-precision sensory data can be further utilized by other applications. 

Robotic trajectory generation is another field that wildly uses mathematics equations to simulate the target object. One approach is the optimal rough terrain trajectory framework [HK07], which solves the optimal path for wheeled mobile robots navigating through uneven and rough terrains like the moon. The method performs physical simulation to solve realistic physical data of the position, velocity, and acceleration but doesn't account for obstacles or collisions. Trajectory Generation for Mobile Robots in Dynamic Environments [BHK21] using Nonlinear Model Predictive Control (NMPC) combines long-range planning with dynamic obstacle avoidance and real-time adaptability. Both methods excel in their specific applications and their focus on individual sensor modalities to simulate and limit their integration into broader scenarios of IoT systems. 

Machine Learning Approaches 

Machine Learning approaches are useful to automatically generate synthetic data from existing data. These approaches, particularly built to deal with time-series data collected by sensors in IoT systems, involve traditional machine learning models and deep learning models. Traditional machine learning techniques like Synthetic Data with Nested Markov Chain [PAP23], combine synthetic data generation techniques with nested Markov chain to model complex dependencies. For example, in the context of smart lighting control, a nested Markov chain can model the probabilities of different lighting conditions and occupant behaviors over time to optimize energy efficiency. 

Deep Learning models utilize multiple layers of neural networks to optimize a loss function to a target. These methods can be regarded as approximating functions from input to output with arbitrary precision with the sufficient large scale of the Neural Network. These models, particularly generative ones, play a significant role in tasks such as time series generation, prediction, and hybrid approaches that combine simulation and Deep Learning. One approach involves deep learning generative models that create synthetic data from random distributions. These models learn to map random distributions to the distribution of real-world data. Generative models have been pivotal in producing synthetic time series data, images, and even more complex forms of data.  

Using deep learning to generate synthetic data can be understood within a hierarchical framework. At the base of this hierarchy is the physical world, IoT sensors capture various forms of data such as images, videos, and 3D models. These sensors serve as the starting point for generating data, which is then transmitted and processed by IoT networks. As the physical sensor data is collected and processed, applications can make predictions and decisions based on this information, thus forming the foundation of IoT systems. Physical data generation becomes a fundamental start of this hierarchy with raw sensory data. For image sensory data generation, one method is Generative Adversarial Networks (GAN) [ACB17] that contains a generator and a discriminator work together to improve image quality iteratively. Similarly, Stable Diffusion [RBL22] uses convolutional neural networks for compression and denoising models to produce high-quality images efficiently. Then for video generations, Grid-Diffusion [LKK24] presents a novel approach to generating videos from text descriptions by creating keyframes and interpolating between them. Other models such as Free-bloom [HFS24], combine Large Language Models (LLMs) with diffusion models to maintain consistency in video generation. Theseframeworks show the growing ability of deep learning models to handle complex data types like videos, guided by textual inputs. 

In physical scenarios, 3D models are used as objects and interact with other objects and environments. Make-It-3D [TWZ23] uses prior knowledge from a 2D diffusion model as 3D-aware supervision for 3D creation. It follows a two-stage optimization: firstly, optimizing a Neural Radiance Field (NeRF) with image constraints, then transforming the model into textured point clouds. Similarly, the Zero-1-to-3 [LWH23] approach leverages geometric priors from large-scale diffusion models for novel view synthesis. It uses a conditional diffusion model to control camera viewpoints and generate new object images, followed by 3D reconstruction to create a 3D triangle mesh.  

As we focus more closely on IoT sensor data, the generative method is also prevalently used for data augmentation. For example, GAN-based data augmentation [LDQ21] strategy for sensor anomaly detection by generating synthetic anomalies. This can help overcome the limitations of small or imbalanced datasets. Wi-Fi See It All [LLY20] involves the use of GAN to enhance Wi-Fi imaging technology that leverages the penetration and reflection characteristics of wireless signals to perceive and image the environment.  

Network data generation is another area where deep learning shows promise. Synthetic attack [KS23] proposed a framework for network intrusion detection systems (NIDS) with GAN used as data augmentation. This approach reduces the dependency on real-world data and offers a more flexible and ethically convenient model-building process. Other models, like TimeVAE [DFW21] and TimeTransformer [LWL24], utilize the generative model to generate time series data, capturing trends and seasonality. Both methods can generate synthetic data, yet the precision and flexibility in different settings are still problems.  

On the application layer, some method utilizing generative methods for privacy preservation is proposed for Federated Learning. GFL [CZL23] use GAN with differential privacy to generate data while preserving privacy during server communication. Social GANs (Zhang, Wu, Dong, & Liu, 2021), another method, can learn pedestrian walking patterns from images to generate realistic human trajectories. Combining simulation and machine learning is another emerging trend in synthetic data generation. These hybrid methods integrate simulation techniques with large language models and vision large models to generate richer, more contextually informed data. For example, RFGen [CZ23] uses large models to generate physical scenes and then simulates radio frequency responses based on these synthetic scenes, marking a significant advancement in creating realistic and contextually informed synthetic data. 

In Appex B, you can find a list of a detailed comparison of related articles for synthetic data generation in tabular format, for both simulation and machine learning approaches.  
